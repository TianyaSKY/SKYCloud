import json
import logging
import os
from collections import defaultdict
from operator import itemgetter

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from sqlalchemy import text

from app.extensions import db
from app.services.model_config import get_chat_model_config, get_embedding_model_config
from app.services.query_rewrite import (
    build_multi_queries,
    build_retrieval_query,
    format_keyword_dimensions,
    require_keyword_dimensions,
    RewriteKeywordDimensions,
)
from app.services.rerank_service import rerank_documents

logger = logging.getLogger(__name__)


def _env_int(name: str, default: int) -> int:
    try:
        value = int(os.getenv(name, str(default)))
        return value if value > 0 else default
    except (TypeError, ValueError):
        return default


RAG_VECTOR_FETCH_K = _env_int("RAG_VECTOR_FETCH_K", 20)
RAG_MULTI_QUERY_MAX_QUERIES = _env_int("RAG_MULTI_QUERY_MAX_QUERIES", 6)
RAG_RRF_K = _env_int("RAG_RRF_K", 60)
RAG_FUSION_TOP_K = _env_int("RAG_FUSION_TOP_K", 30)


def get_chat_model():
    config = get_chat_model_config()

    return ChatOpenAI(
        api_key=config["key"],
        base_url=config["api"],
        model=config["model"],
        temperature=0,
    )


def get_embeddings_model():
    config = get_embedding_model_config()

    return OpenAIEmbeddings(
        api_key=config["key"],
        base_url=config["api"],
        model=config["model"],
    )


def _vector_search_docs(
        query_text: str,
        user_id: int,
        embeddings: OpenAIEmbeddings,
        limit: int,
) -> list[Document]:
    query_vector = embeddings.embed_query(query_text)[:1024]

    sql = text("""
               SELECT id, name, description, mime_type, (vector_info <=> :vector) AS distance
                FROM files
                WHERE description IS NOT NULL
                  AND description != ''
           AND uploader_id = :user_id
                ORDER BY vector_info <=> :vector
                   LIMIT :limit
               """)

    results = db.session.execute(
        sql,
        {"vector": str(query_vector), "user_id": user_id, "limit": RAG_VECTOR_FETCH_K},
    ).fetchall()
    docs: list[Document] = []
    for rank, row in enumerate(results, start=1):
        content = f"æ–‡ä»¶å: {row[1]}\næè¿°: {row[2]}"
        metadata = {
            "id": row[0],
            "name": row[1],
            "mime_type": row[3],
            "distance": float(row[4]) if row[4] is not None else None,
            "rank": rank,
            "query_text": query_text,
        }
        docs.append(Document(page_content=content, metadata=metadata))
    return docs


def _fuse_docs_with_rrf(
        result_sets: list[list[Document]],
        rrf_k: int,
        top_k: int,
) -> list[Document]:
    if not result_sets:
        return []

    scores: dict[int, float] = defaultdict(float)
    doc_map: dict[int, Document] = {}

    for docs in result_sets:
        for rank, doc in enumerate(docs, start=1):
            doc_id = int(doc.metadata["id"])
            scores[doc_id] += 1.0 / (rrf_k + rank)
            if doc_id not in doc_map:
                doc_map[doc_id] = doc

    ranked_ids = sorted(
        scores.keys(),
        key=lambda doc_id: (-scores[doc_id], doc_map[doc_id].metadata.get("distance", float("inf"))),
    )
    fused_docs: list[Document] = []
    for doc_id in ranked_ids[:top_k]:
        doc = doc_map[doc_id]
        doc.metadata["rrf_score"] = scores[doc_id]
        fused_docs.append(doc)
    return fused_docs


def custom_db_retriever(query_text: str, user_id: int):
    """å•æŸ¥è¯¢å‘é‡æ£€ç´¢ï¼Œä¿ç•™å…¼å®¹ã€‚"""
    embeddings = get_embeddings_model()
    docs = _vector_search_docs(query_text, user_id, embeddings, RAG_VECTOR_FETCH_K)
    docs = rerank_documents(query_text, docs)
    logger.info(f"å•æŸ¥è¯¢æ£€ç´¢ç»“æœ: {len(docs)}")
    return docs


def multi_query_db_retriever(
        question: str,
        user_id: int,
        dimensions: RewriteKeywordDimensions,
):
    queries = build_multi_queries(
        question,
        dimensions,
        max_queries=RAG_MULTI_QUERY_MAX_QUERIES,
    )
    if not queries and question.strip():
        queries = [question.strip()]

    embeddings = get_embeddings_model()
    result_sets: list[list[Document]] = []
    for query_text in queries:
        try:
            docs = _vector_search_docs(
                query_text,
                user_id,
                embeddings,
                RAG_VECTOR_FETCH_K,
            )
            if docs:
                result_sets.append(docs)
        except Exception as exc:
            logger.warning(f"Multi-query recall failed for query='{query_text}': {exc}")

    fused_docs = _fuse_docs_with_rrf(result_sets, rrf_k=RAG_RRF_K, top_k=RAG_FUSION_TOP_K)
    retrieval_query = build_retrieval_query(question, dimensions)
    reranked_docs = rerank_documents(retrieval_query, fused_docs)
    logger.info(
        f"å¤šæŸ¥è¯¢èåˆæ£€ç´¢å®Œæˆ: queries={len(queries)}, raw_sets={len(result_sets)}, fused={len(fused_docs)}, reranked={len(reranked_docs)}"
    )
    return reranked_docs


def format_docs(docs):
    formatted = []
    for doc in docs:
        m = doc.metadata
        info = f"[æ–‡ä»¶: {m['name']} (ID: {m['id']})]\n{doc.page_content}"
        # å¦‚æœæ˜¯å›¾ç‰‡ï¼Œæç¤º AI å¯ä»¥ä½¿ç”¨ç‰¹å®šè¯­æ³•å¼•ç”¨
        if m.get('mime_type', '').startswith('image/'):
            info += f"\n(è¿™æ˜¯ä¸€å¼ å›¾ç‰‡ï¼Œä½ å¯ä»¥ä½¿ç”¨ Markdown è¯­æ³•å±•ç¤ºå®ƒ: ![å›¾ç‰‡å](/api/files/{m['id']}/download))"
        formatted.append(info)
    return "\n\n---\n\n".join(formatted)


def format_history(history):
    if isinstance(history, list):
        return "\n".join([f"{h.get('role', 'user')}: {h.get('content', '')}" for h in history])
    return str(history) if history else ""


def retrieve_docs_with_rewrite(payload: dict):
    question = str(payload.get("question", "") or "")
    rewrite_output = payload.get("rewrite_output")
    current_user_id = int(payload["user_id"])

    dimensions = require_keyword_dimensions(rewrite_output)
    return multi_query_db_retriever(question, current_user_id, dimensions)


async def generate_chat_events(user_id, query: str, history: list):
    """å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äº SSE æµå¼è¾“å‡º"""
    llm = get_chat_model()
    formatted_history = format_history(history)

    # å…³é”®è¯é‡å†™é“¾ï¼šå¤šç»´åº¦æå–ï¼Œç»Ÿä¸€è¾“å‡º JSONï¼Œä¾¿äºä¸‹æ¸¸æ£€ç´¢æ‹¼æ¥
    rewrite_prompt = ChatPromptTemplate.from_template("""
è¯·ä»ç”¨æˆ·é—®é¢˜ä¸­æå–æ£€ç´¢å…³é”®è¯ã€‚
è¾“å‡ºè¦æ±‚ï¼š
1. ä¸»é¢˜è¯ï¼ˆtopic_termsï¼‰
2. å®ä½“è¯ï¼ˆentity_termsï¼‰
3. æ—¶é—´è¯ï¼ˆtime_termsï¼‰
4. æ–‡ä»¶ç±»å‹è¯ï¼ˆfile_type_termsï¼‰
5. åŠ¨ä½œè¯ï¼ˆaction_termsï¼‰
6. åŒä¹‰æ‰©å±•è¯ï¼ˆsynonym_termsï¼‰

è¦æ±‚ï¼š
1. æ¯ä¸ªå­—æ®µéƒ½åº”è¯¥æ˜¯è‹±æ–‡çŸ­è¯­æ•°ç»„ã€‚
2. æ— å†…å®¹æ—¶è¿”å›ç©ºæ•°ç»„ã€‚
3. ä¸è¦ç¼–é€ æ— å…³è¯ã€‚

é—®é¢˜ï¼š{question}
""")
    rewrite_llm = llm.with_structured_output(RewriteKeywordDimensions)
    rewriter = (rewrite_prompt | rewrite_llm).with_config({"run_name": "keyword_gen"})

    # æœ€ç»ˆå›ç­”çš„æç¤ºè¯æ¨¡æ¿ï¼šä½¿ç”¨ä¸­æ–‡æç¤ºè¯
    answer_prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªäº‘ç›˜åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

### æŒ‡ä»¤è¦æ±‚ï¼š
1. è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
2. **ä¸¥ç¦åœ¨å›ç­”å¼€å¤´è¾“å‡ºæœç´¢å…³é”®è¯ã€‚** ç›´æ¥å¼€å§‹ä½ çš„å›ç­”ã€‚
3. **å±•ç¤ºå›¾ç‰‡ï¼š** å¦‚æœå‚è€ƒä¿¡æ¯ä¸­æœ‰å›¾ç‰‡ä¸”ç›¸å…³ï¼Œå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å±•ç¤ºï¼š
   `![å›¾ç‰‡æè¿°](/api/files/æ–‡ä»¶ID/download)`
   æ³¨æ„ï¼šæ–‡ä»¶ID å¿…é¡»æ›¿æ¢ä¸ºå‚è€ƒä¿¡æ¯ä¸­æä¾›çš„ IDã€‚
4. ä¿æŒå›ç­”ç®€æ´ä¸“ä¸šã€‚

### å†å²å¯¹è¯ï¼š
{history}

### å‚è€ƒä¿¡æ¯ï¼š
{context}

### é—®é¢˜ï¼š
{question}
""")

    # æœ€ç»ˆå›ç­”é“¾
    answer_chain = (
        answer_prompt | 
        llm.with_config({"run_name": "final_answer_model", "tags": ["final_answer"]}) | 
        StrOutputParser()
    ).with_config({"run_name": "final_answer_chain"})

    rag_chain = (
            RunnableParallel({
                "context": {
                               "rewrite_output": rewriter,
                               "question": itemgetter("question"),
                               "user_id": itemgetter("user_id"),
                           } | RunnableLambda(retrieve_docs_with_rewrite).with_config(
                    {"run_name": "custom_db_retriever"}
                ) | format_docs,
                "question": itemgetter("question"),
                "history": itemgetter("history")
            })
            | answer_chain
    )

    try:
        async for event in rag_chain.astream_events(
                {"question": query, "history": formatted_history, "user_id": user_id},
                version="v2"
        ):
            kind = event["event"]

            # å¤„ç†å…³é”®è¯ç”Ÿæˆï¼ˆä»…åœ¨é“¾ç»“æŸæ—¶å‘é€å®Œæ•´å…³é”®è¯ï¼‰
            if kind == "on_chain_end" and event["name"] == "keyword_gen":
                rewrite_output = event["data"].get("output")
                dimensions = require_keyword_dimensions(rewrite_output)
                keywords = format_keyword_dimensions(dimensions)
                yield f"data: {json.dumps({'type': 'keywords', 'content': keywords})}\n\n"

            # å¤„ç†æœ€ç»ˆå›ç­”çš„æµï¼šé€šè¿‡ run_name 'final_answer_model' ç¡®ä¿ä¸åŒ…å«å…³é”®è¯ç”Ÿæˆçš„ token
            elif kind == "on_chat_model_stream" and event["name"] == "final_answer_model":
                content = event["data"]["chunk"].content
                if content:
                    yield f"data: {json.dumps({'type': 'token', 'content': content})}\n\n"

            # å¤„ç†çŠ¶æ€
            elif kind == "on_retriever_start" or (kind == "on_chain_start" and event["name"] == "custom_db_retriever"):
                yield f"data: {json.dumps({'type': 'status', 'content': 'ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡ä»¶...'})}\n\n"

    except Exception as e:
        logger.error(f"Chat error: {e}", exc_info=True)
        yield f"data: {json.dumps({'type': 'status', 'content': f'âŒ é”™è¯¯: {str(e)}'})}\n\n"
