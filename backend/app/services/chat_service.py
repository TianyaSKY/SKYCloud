import json
import logging
from operator import itemgetter

from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from sqlalchemy import text

from app.extensions import db
from app.services.sys_dict_service import get_sys_dict_by_key

logger = logging.getLogger(__name__)

def get_chat_model():
    api_url = get_sys_dict_by_key('chat_api_url').value
    api_key = get_sys_dict_by_key('chat_api_key').value
    model_name = get_sys_dict_by_key('chat_api_model').value
    
    return ChatOpenAI(
        api_key=api_key,
        base_url=api_url,
        model=model_name,
        temperature=0,
    )

def get_embeddings_model():
    api_url = get_sys_dict_by_key('emb_api_url').value
    api_key = get_sys_dict_by_key('emb_api_key').value
    model_name = get_sys_dict_by_key('emb_model_name').value
    
    return OpenAIEmbeddings(
        api_key=api_key,
        base_url=api_url,
        model=model_name
    )

def custom_db_retriever(query_text: str, user_id: int):
    """è‡ªå®šä¹‰æ£€ç´¢å™¨ï¼Œä½¿ç”¨ SQLAlchemy æ‰§è¡Œå‘é‡æœç´¢"""
    embeddings = get_embeddings_model()
    query_vector = embeddings.embed_query(query_text)[:1536]
    
    sql = text("""
        SELECT id, name, description, mime_type
        FROM files
        WHERE description IS NOT NULL
          AND description != ''
          AND uploader_id = :user_id
        ORDER BY vector_info <=> :vector
        LIMIT 20
    """)
    
    results = db.session.execute(sql, {"vector": str(query_vector), "user_id": user_id}).fetchall()
    docs = []
    for row in results:
        content = f"æ–‡ä»¶å: {row[1]}\næè¿°: {row[2]}"
        metadata = {"id": row[0], "name": row[1], "mime_type": row[3]}
        docs.append(Document(page_content=content, metadata=metadata))
        
    logger.info(f"æ•°æ®åº“æ£€ç´¢åˆ° {len(docs)} æ¡ç»“æœ")
    return docs

def format_docs(docs):
    formatted = []
    for doc in docs:
        m = doc.metadata
        info = f"[æ–‡ä»¶: {m['name']} (ID: {m['id']})]\n{doc.page_content}"
        # å¦‚æœæ˜¯å›¾ç‰‡ï¼Œæç¤º AI å¯ä»¥ä½¿ç”¨ç‰¹å®šè¯­æ³•å¼•ç”¨
        if m.get('mime_type', '').startswith('image/'):
            info += f"\n(è¿™æ˜¯ä¸€å¼ å›¾ç‰‡ï¼Œä½ å¯ä»¥ä½¿ç”¨ Markdown è¯­æ³•å±•ç¤ºå®ƒ: ![å›¾ç‰‡](/api/file/download/{m['id']}))"
        formatted.append(info)
    return "\n\n---\n\n".join(formatted)

def format_history(history):
    if isinstance(history, list):
        return "\n".join([f"{h.get('role', 'user')}: {h.get('content', '')}" for h in history])
    return str(history) if history else ""

async def generate_chat_events(user_id, query: str, history: list):
    """å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œç”¨äº SSE æµå¼è¾“å‡º"""
    llm = get_chat_model()
    formatted_history = format_history(history)

    # å…³é”®è¯é‡å†™é“¾ï¼šä½¿ç”¨ä¸­æ–‡æç¤ºè¯ï¼Œä½†è¦æ±‚è¾“å‡ºè‹±æ–‡å…³é”®è¯ï¼ˆæ ¹æ®ä¹‹å‰çš„è¦æ±‚ï¼‰
    rewrite_prompt = ChatPromptTemplate.from_template("è¯·ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–æ ¸å¿ƒæœç´¢å…³é”®è¯ï¼š{question}ã€‚ä»…è¾“å‡ºè‹±æ–‡å…³é”®è¯ã€‚")
    rewriter = (rewrite_prompt | llm | StrOutputParser()).with_config({"run_name": "keyword_gen"})

    # æœ€ç»ˆå›ç­”çš„æç¤ºè¯æ¨¡æ¿ï¼šä½¿ç”¨ä¸­æ–‡æç¤ºè¯
    answer_prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªäº‘ç›˜åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

1. è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚
2. å¦‚æœå‚è€ƒä¿¡æ¯ä¸­æœ‰å›¾ç‰‡æ–‡ä»¶ï¼Œä¸”ä¸é—®é¢˜ç›¸å…³ï¼Œè¯·åœ¨å›ç­”ä¸­ä½¿ç”¨ Markdown è¯­æ³• `![å›¾ç‰‡å](/api/file/download/æ–‡ä»¶ID)` å±•ç¤ºå›¾ç‰‡ã€‚
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ ¹æ®å·²çŸ¥å†…å®¹å›ç­”ã€‚
4. ä¿æŒå›ç­”ç®€æ´ä¸“ä¸šã€‚

å†å²å¯¹è¯ï¼š
{history}

å‚è€ƒä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}
""")

    # æœ€ç»ˆå›ç­”é“¾
    answer_chain = (answer_prompt | llm | StrOutputParser()).with_config({"run_name": "final_answer"})

    rag_chain = (
            RunnableParallel({
                "context": {
                    "query_text": rewriter,
                    "user_id": itemgetter("user_id")
                } | RunnableLambda(lambda x: custom_db_retriever(x["query_text"], x["user_id"])) | format_docs,
                "question": itemgetter("question"),
                "history": itemgetter("history")
            })
            | answer_chain
    )

    try:
        async for event in rag_chain.astream_events(
                {"question": query, "history": formatted_history, "user_id": user_id},
                version="v2"
        ):
            kind = event["event"]
            
            # å¤„ç†å…³é”®è¯ç”Ÿæˆ
            if kind == "on_chain_end" and event["name"] == "keyword_gen":
                keywords = event["data"]["output"]
                yield f"data: {json.dumps({'type': 'keywords', 'content': keywords})}\n\n"
            
            # å¤„ç†æœ€ç»ˆå›ç­”çš„æµ
            elif kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    yield f"data: {json.dumps({'type': 'token', 'content': content})}\n\n"
            
            # å¤„ç†çŠ¶æ€
            elif kind == "on_retriever_start" or (kind == "on_chain_start" and event["name"] == "custom_db_retriever"):
                yield f"data: {json.dumps({'type': 'status', 'content': 'ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡ä»¶...'})}\n\n"

    except Exception as e:
        logger.error(f"Chat error: {e}", exc_info=True)
        yield f"data: {json.dumps({'type': 'status', 'content': f'âŒ é”™è¯¯: {str(e)}'})}\n\n"
